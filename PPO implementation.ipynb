{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO algorithm Implementation from scratch to solve \"Lunar Lander\" problem (Continuous version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import torch.nn as nn\n",
    "from scipy.stats import norm\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pytorch-compatible density function for the 1-D distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_density(x, mu, sigma):\n",
    "    \"\"\" Input are tensors\"\"\"\n",
    "    \n",
    "    factor= 1./(torch.sqrt(2*np.pi*sigma))\n",
    "    expo = torch.exp(-((x-mu)**2)/(2*sigma))\n",
    "    return factor*expo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our main classes (Replay buffer, Networks, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    ''' Base agent class\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): actions dimensionality\n",
    "\n",
    "        Attributes:\n",
    "            n_actions (int): where we store the dimensionality of an action\n",
    "    '''\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state: np.ndarray):\n",
    "        ''' Performs a forward computation '''\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        ''' Performs a backward pass on the network '''\n",
    "        pass\n",
    "\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "    \n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "\n",
    "        # Sample without replacement the indices of the experiences\n",
    "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
    "        # number of elements to sample and replacement.\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size=n,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Using the indices that we just sampled build a list of chosen experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "        return zip(*batch)\n",
    "\n",
    "### Neural Network ###\n",
    "class actor_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400) #Shared Input Layer\n",
    "        \n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_mean = nn.Linear(400, 200)\n",
    "        self.output_mean= nn.Linear(200, output_size)\n",
    "        self.act_mean = nn.Tanh()\n",
    "        \n",
    "        self.input_sigma= nn.Linear(400,200)\n",
    "        self.output_sigma= nn.Linear(200, output_size)\n",
    "        self.act_sigma= nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1= self.input_layer_activation(l1)\n",
    "        \n",
    "        #Head mean\n",
    "        mean= self.input_mean(l1)\n",
    "        mean= self.input_layer_activation(mean)\n",
    "        mean= self.output_mean (mean)\n",
    "        mean= self.act_mean(mean)\n",
    "        \n",
    "        #Head sigma\n",
    "        sigma= self.input_sigma(l1)\n",
    "        sigma= self.input_layer_activation(sigma)\n",
    "        sigma= self.output_sigma(sigma)\n",
    "        sigma= self.act_sigma(sigma)\n",
    "        \n",
    "        return mean, sigma\n",
    "    \n",
    "class critic_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_layer1 = nn.Linear(400, 200)\n",
    "        self.output_layer= nn.Linear(200, output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        l1 = self.input_layer1(l1)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        out= self.output_layer(l1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out    \n",
    "    \n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
    "        ''' Compute a random action in [-1, 1]\n",
    "\n",
    "            Returns:\n",
    "                action (np.ndarray): array of float values containing the\n",
    "                    action. The dimensionality is equal to self.n_actions from\n",
    "                    the parent class Agent\n",
    "        '''\n",
    "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we fix our parameters and we launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/1600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed on kwargs for b2BodyDef.position: Converting from sequence to b2Vec2, expected int/float arguments index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\Box2D\\Box2D.py:108\u001b[0m, in \u001b[0;36m_init_kwargs\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[1;31mTypeError\u001b[0m: Converting from sequence to b2Vec2, expected int/float arguments index 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 78\u001b[0m\n\u001b[0;32m     71\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# The next line takes permits you to take an action in the RL environment\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# env.step(action) returns 4 variables:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# (1) next state; (2) reward; (3) done variable; (4) additional stuff\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#COMPUTE Relevant measures\u001b[39;00m\n\u001b[0;32m     82\u001b[0m G\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    216\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    217\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:544\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    537\u001b[0m oy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtip[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m dispersion[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m side[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m dispersion[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m direction \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_AWAY \u001b[38;5;241m/\u001b[39m SCALE\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m impulse_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m ox \u001b[38;5;241m-\u001b[39m tip[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m17\u001b[39m \u001b[38;5;241m/\u001b[39m SCALE,\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m oy \u001b[38;5;241m+\u001b[39m tip[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_HEIGHT \u001b[38;5;241m/\u001b[39m SCALE,\n\u001b[0;32m    543\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_particle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpulse_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpulse_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_power\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[0;32m    546\u001b[0m     (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[0;32m    547\u001b[0m     impulse_pos,\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m )\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[0;32m    551\u001b[0m     (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[0;32m    552\u001b[0m     impulse_pos,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    554\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:423\u001b[0m, in \u001b[0;36mLunarLander._create_particle\u001b[1;34m(self, mass, x, y, ttl)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_particle\u001b[39m(\u001b[38;5;28mself\u001b[39m, mass, x, y, ttl):\n\u001b[1;32m--> 423\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateDynamicBody\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mangle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixtures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixtureDef\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcircleShape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSCALE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfriction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcategoryBits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0x0100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmaskBits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0x001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# collide only with ground\u001b[39;49;00m\n\u001b[0;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestitution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m     p\u001b[38;5;241m.\u001b[39mttl \u001b[38;5;241m=\u001b[39m ttl\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparticles\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\Box2D\\Box2D.py:3134\u001b[0m, in \u001b[0;36mb2World.CreateDynamicBody\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;124;03mCreate a single dynamic body in the world.\u001b[39;00m\n\u001b[0;32m   3129\u001b[0m \n\u001b[0;32m   3130\u001b[0m \u001b[38;5;124;03mAccepts only kwargs to a b2BodyDef. For more information, see\u001b[39;00m\n\u001b[0;32m   3131\u001b[0m \u001b[38;5;124;03mCreateBody and b2BodyDef.\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3133\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m b2_dynamicBody\n\u001b[1;32m-> 3134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateBody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\Box2D\\Box2D.py:3185\u001b[0m, in \u001b[0;36mb2World.CreateBody\u001b[1;34m(self, defn, **kwargs)\u001b[0m\n\u001b[0;32m   3183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected b2BodyDef\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3185\u001b[0m     defn \u001b[38;5;241m=\u001b[39m \u001b[43mb2BodyDef\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m   3187\u001b[0m body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__CreateBody(defn)\n\u001b[0;32m   3189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m defn\u001b[38;5;241m.\u001b[39mfixtures:\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\Box2D\\Box2D.py:2209\u001b[0m, in \u001b[0;36mb2BodyDef.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2208\u001b[0m     _Box2D\u001b[38;5;241m.\u001b[39mb2BodyDef_swiginit(\u001b[38;5;28mself\u001b[39m,_Box2D\u001b[38;5;241m.\u001b[39mnew_b2BodyDef())\n\u001b[1;32m-> 2209\u001b[0m     \u001b[43m_init_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleme\\Desktop\\RL\\Projet\\.venv\\Lib\\site-packages\\Box2D\\Box2D.py:110\u001b[0m, in \u001b[0;36m_init_kwargs\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed on kwargs for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    111\u001b[0m                 \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key, ex))\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed on kwargs for b2BodyDef.position: Converting from sequence to b2Vec2, expected int/float arguments index 0"
     ]
    }
   ],
   "source": [
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_episodes = 1600              # Number of episodes to run for training\n",
    "discount_factor = 0.99        # Value of gamma\n",
    "n_ep_running_average = 50      # Running average of 50 episodes\n",
    "n_actions = len(env.action_space.high)               # Number of available actions\n",
    "dim_state = len(env.observation_space.high)  # State dimensionality\n",
    "epsilon=0.2\n",
    "M=10\n",
    "lr_critic=1e-3\n",
    "lr_actor=1e-5\n",
    "L=10000\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to save episodes reward\n",
    "episode_number_of_steps = []\n",
    "\n",
    "\n",
    "\n",
    "### Filling up Buffer with Random experiences\n",
    "agent = RandomAgent(n_actions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Create network ###\n",
    "\n",
    "actor = actor_net(input_size= dim_state, output_size=n_actions)\n",
    "critic = critic_net(input_size= dim_state, output_size=1)\n",
    "\n",
    "\n",
    "### Create optimizer ###\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "\n",
    "\n",
    "# Training process\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "\n",
    "for episode in EPISODES:\n",
    "    \n",
    "    ### Create Experience replay buffer for the episode ###\n",
    "    buffer = ExperienceReplayBuffer(maximum_length=L)\n",
    "\n",
    "    total_episode_reward=0\n",
    "    state, _ = env.reset()\n",
    "# Reset environment, returns                                     # initial state\n",
    "    done = False                           # Boolean variable used to indicate\n",
    "    t=0                                     #Step of episode \n",
    "    \n",
    "    G=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()                       # Render the environment, remove this\n",
    "                                           # line if you run on Google Colab\n",
    "        # Create state tensor, remember to use single precision (torch.float32)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            states.append(state)\n",
    "\n",
    "            mu, var = actor(torch.tensor([state]))\n",
    "            mu = mu.detach().numpy()\n",
    "            std = torch.sqrt(var).detach().numpy()\n",
    "            action = np.clip(np.random.normal(mu, std), -1, 1).flatten()            \n",
    "            actions.append(action)\n",
    "\n",
    "        \n",
    "\n",
    "        # The next line takes permits you to take an action in the RL environment\n",
    "        # env.step(action) returns 4 variables:\n",
    "        # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        #COMPUTE Relevant measures\n",
    "        G.append(reward)\n",
    "        if len(G)>1:\n",
    "            for i in range(len(G)-1):\n",
    "                G[i]+= (discount_factor**(t-i))*reward\n",
    "\n",
    "        \n",
    "        \n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Append experience to the buffer\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "        \n",
    "        \n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "        t+=1\n",
    "    \n",
    "    #Compute advantage and old_pi and Psi as targets in order to perform training\n",
    "    G= torch.tensor(G, requires_grad=False, dtype=torch.float32)\n",
    "    states= torch.tensor(states, requires_grad=True, dtype=torch.float32)\n",
    "    actions= torch.tensor(actions, requires_grad=False, dtype=torch.float32)\n",
    "    V= critic(states)\n",
    "    V=torch.reshape(V, G.shape)\n",
    "    Psi= G-V.detach()\n",
    "    means, sigmas= actor(states)\n",
    "    val1, val2= normal_density(actions[:,0], means[:,0], sigmas[:,0]), normal_density(actions[:,1], means[:,1], sigmas[:,1])\n",
    "    pi= val1*val2\n",
    "    old_pi=pi.detach()\n",
    "    \n",
    "    for n in range(1,M+1):\n",
    "        #Optimize Critic\n",
    "        critic.train()\n",
    "        \n",
    "        #Compute Targets\n",
    "    \n",
    "        \n",
    "        V= critic(states)\n",
    "        V=torch.reshape(V, G.shape)\n",
    "        \n",
    "        means, sigmas= actor(states)\n",
    "        val1, val2= normal_density(actions[:,0], means[:,0], sigmas[:,0]), normal_density(actions[:,1], means[:,1], sigmas[:,1])\n",
    "        pi= val1*val2\n",
    "        loss_critic= nn.functional.mse_loss(\n",
    "                            V, G.detach())\n",
    "        \n",
    "        optimizer_critic.zero_grad()\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss_critic.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.)\n",
    "\n",
    "        # Perform backward pass (backpropagation)\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        #Optimize Actor\n",
    "        \n",
    "        r= torch.div(pi, old_pi)\n",
    "        \n",
    "        c_eps= torch.max(torch.min(r,torch.tensor(1+epsilon,\n",
    "                                                  requires_grad=False, dtype=torch.float32)), \n",
    "                         torch.tensor(1-epsilon, requires_grad=False, dtype=torch.float32))\n",
    "        \n",
    "        Psi=Psi.detach()\n",
    "        \n",
    "        \n",
    "        actor.train()\n",
    "        \n",
    "        loss_actor= -torch.mean(torch.min(r*Psi,c_eps*Psi))\n",
    "        \n",
    "        optimizer_actor.zero_grad()\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss_actor.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.)\n",
    "\n",
    "        # Perform backward pass (backpropagation)\n",
    "        optimizer_actor.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Add rewards and number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "        episode, total_episode_reward, t,\n",
    "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Close all the windows\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Solution\n",
    "\n",
    "Here we check the validity of our model. We achieve high performances with the above parameters (200+ average reward over 50 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 0:   0%|                                                                                | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network model: actor_net(\n",
      "  (input_layer): Linear(in_features=8, out_features=400, bias=True)\n",
      "  (input_layer_activation): ReLU()\n",
      "  (input_mean): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (output_mean): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (act_mean): Tanh()\n",
      "  (input_sigma): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (output_sigma): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (act_sigma): Sigmoid()\n",
      ")\n",
      "Checking solution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 49: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [00:20<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy achieves an average total reward of 161.5 +/- 32.5 with confidence 95%.\n",
      "Your policy passed the test!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from PPO_agent import actor_net\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = actor\n",
    "    print('Network model: {}'.format(model))\n",
    "except:\n",
    "    print('File neural-network-3-actor.pth not found!')\n",
    "    exit(-1)\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_EPISODES = 50            # Number of episodes to run for trainings\n",
    "CONFIDENCE_PASS = 125\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to store episodes reward\n",
    "\n",
    "# Simulate episodes\n",
    "print('Checking solution...')\n",
    "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
    "for i in EPISODES:\n",
    "    EPISODES.set_description(\"Episode {}\".format(i))\n",
    "    # Reset enviroment data\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0.\n",
    "    while not done:\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        mu, var = model(torch.tensor([state]))\n",
    "        mu = mu.detach().numpy()\n",
    "        std = torch.sqrt(var).detach().numpy()\n",
    "        actions = np.clip(np.random.normal(mu, std), -1, 1).flatten()\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Append episode reward\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "avg_reward = np.mean(episode_reward_list)\n",
    "confidence = np.std(episode_reward_list) * 1.96 / np.sqrt(N_EPISODES)\n",
    "\n",
    "\n",
    "print('Policy achieves an average total reward of {:.1f} +/- {:.1f} with confidence 95%.'.format(\n",
    "                avg_reward,\n",
    "                confidence))\n",
    "\n",
    "if avg_reward - confidence > CONFIDENCE_PASS:\n",
    "    print('Your policy passed the test!')\n",
    "else:\n",
    "    print(\"Your policy did not pass the test! The average reward of your policy needs to be greater than {} with 95% confidence\".format(CONFIDENCE_PASS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
